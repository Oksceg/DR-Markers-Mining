{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Эксперимент №1\n",
        "В данном эксперименте производится попытка извлечения n-грамм по определенным заданным критериям, которые свойственны дискурсивным маркерам"
      ],
      "metadata": {
        "id": "LltJsE-Es0Jg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Скачиваем *spacy_udpipe*, импортируем нужные для работы модули, получаем доступ к гугл-диску, в котором лежат новостные файлы"
      ],
      "metadata": {
        "id": "QLewzWibshdq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZRSiDYNQLSr",
        "outputId": "c66a5731-5544-4d6c-8e2c-0cade434698f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy_udpipe\n",
            "  Downloading spacy_udpipe-1.0.0-py3-none-any.whl (11 kB)\n",
            "Collecting ufal.udpipe>=1.2.0\n",
            "  Downloading ufal.udpipe-1.2.0.3.tar.gz (304 kB)\n",
            "\u001b[K     |████████████████████████████████| 304 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting spacy<4.0.0,>=3.0.0\n",
            "  Downloading spacy-3.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 63.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.21.6)\n",
            "Collecting thinc<8.1.0,>=8.0.14\n",
            "  Downloading thinc-8.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (653 kB)\n",
            "\u001b[K     |████████████████████████████████| 653 kB 48.9 MB/s \n",
            "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 71.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (4.64.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (21.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.0.6)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 40.2 MB/s \n",
            "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.9.1)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
            "Collecting typing-extensions<4.0.0.0,>=3.7.4\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.6)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.9\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n",
            "\u001b[K     |████████████████████████████████| 457 kB 72.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.8)\n",
            "Collecting smart-open<6.0.0,>=5.0.0\n",
            "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2021.10.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.0.1)\n",
            "Building wheels for collected packages: ufal.udpipe\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp37-cp37m-linux_x86_64.whl size=5626810 sha256=d17bc38c58f50a8ea724a6f932e65639469457acbf368f8921f0d936ebf425e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/b5/8e/3da091629a21ce2d10bf90759d0cb034ba10a5cf7a01e83d64\n",
            "Successfully built ufal.udpipe\n",
            "Installing collected packages: typing-extensions, catalogue, typer, srsly, smart-open, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, ufal.udpipe, spacy, spacy-udpipe\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.2.0\n",
            "    Uninstalling typing-extensions-4.2.0:\n",
            "      Successfully uninstalled typing-extensions-4.2.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 6.0.0\n",
            "    Uninstalling smart-open-6.0.0:\n",
            "      Successfully uninstalled smart-open-6.0.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n",
            "Successfully installed catalogue-2.0.7 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 smart-open-5.2.1 spacy-3.3.0 spacy-legacy-3.0.9 spacy-loggers-1.0.2 spacy-udpipe-1.0.0 srsly-2.4.3 thinc-8.0.15 typer-0.4.1 typing-extensions-3.10.0.2 ufal.udpipe-1.2.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy_udpipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AHaY5aUkZZe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44331b96-4243-4ade-c12b-bf220b44aa3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded pre-trained UDPipe model for 'ru' language\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import collections\n",
        "import spacy_udpipe\n",
        "from collections import Counter\n",
        "spacy_udpipe.download(\"ru\")\n",
        "nlp = spacy_udpipe.load(\"ru\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBa43oLPJgop",
        "outputId": "1ac90da9-b0e0-45bb-a6cb-79b1b45edb5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Извлечение"
      ],
      "metadata": {
        "id": "q_gT4tsgtiGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сначала пропишем функции:"
      ],
      "metadata": {
        "id": "EkzbkZ_UuAsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_first_4grams(t):\n",
        "    bgn_4grams_lists=[sent.lower().split()[:4] for sent in t.split(\".\")]\n",
        "\n",
        "    #сокращаем 4-граммы в зависимости от запятых: сохраняем всю часть до запятой и одно слово после нее\n",
        "    bgn_grams = []\n",
        "    for gram in bgn_4grams_lists:\n",
        "        gram_to_list = \"\"\n",
        "        for w in gram:\n",
        "            if \",\" in w:\n",
        "                gram_to_list = \" \".join(gram[:gram.index(w)+2])\n",
        "            else:\n",
        "                gram_to_list = \" \".join(gram)\n",
        "        bgn_grams.append(gram_to_list)\n",
        "    return bgn_grams\n",
        "\n",
        "def get_grams_with_preps(grams):\n",
        "    #набираем выражения с предлогом\n",
        "    prep_grams = []\n",
        "    for gram in grams:\n",
        "        doc = nlp(gram)\n",
        "        for token in doc:\n",
        "            if token.pos_ == \"ADP\":\n",
        "                prep_grams.append(doc)\n",
        "    return prep_grams\n",
        "\n",
        "def get_th_grams(grams_):\n",
        "    #набираем выражения с местоимениями \"это\" и \"то\"\n",
        "    th_grams = []\n",
        "    for gram in grams_:\n",
        "        doc = nlp(gram)\n",
        "        for token in doc:\n",
        "            if token.lemma_ == \"это\" or token.lemma_ == \"то\":\n",
        "                if token.pos_ == \"PRON\":\n",
        "                    th_grams.append(doc)\n",
        "    return th_grams"
      ],
      "metadata": {
        "id": "qv7hWltdoEnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Открываем файлы один за другим, чистим тексты, оставляя точки, запятые, дефисы. Используем прописанные выше функции:"
      ],
      "metadata": {
        "id": "AWgYBslIuSVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "with open(\"/content/drive/MyDrive/news_texts/2007.txt\", \"r\", encoding = \"utf-8\") as aa:\n",
        "    text_0 = aa.read()\n",
        "    cl_text0 = re.sub(r'[^\\w\\s\\.\\,\\-]','', text_0)\n",
        "    cl_text0 = cl_text0.replace(' - ', ' ')\n",
        "grams0 = get_first_4grams(cl_text0)\n",
        "grams_preps0 = get_th_grams(get_grams_with_preps(grams0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NgK8kcfgOth",
        "outputId": "5d94cb56-d58f-417f-b484-281fa64d00f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 20min 41s, sys: 11.4 s, total: 20min 52s\n",
            "Wall time: 20min 49s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GORBDpwsBh6A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c17a215-f1cc-46f8-9833-2962489959fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 25min 17s, sys: 12.2 s, total: 25min 29s\n",
            "Wall time: 25min 21s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "with open(\"/content/drive/MyDrive/news_texts/2008.txt\", \"r\", encoding = \"utf-8\") as a:\n",
        "    text_1 = a.read()\n",
        "    cl_text1 = re.sub(r'[^\\w\\s\\.\\,\\-]','', text_1)\n",
        "    cl_text1 = cl_text1.replace(' - ', ' ')\n",
        "grams1 = get_first_4grams(cl_text1)\n",
        "grams_preps1 = get_th_grams(get_grams_with_preps(grams1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDZwsidBJn_0",
        "outputId": "c807e703-91bf-4dd2-c78a-69d94c85c534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 25min 16s, sys: 12 s, total: 25min 28s\n",
            "Wall time: 25min 21s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "with open(\"/content/drive/MyDrive/news_texts/2009.txt\", \"r\", encoding = \"utf-8\") as b:\n",
        "    text_2 = b.read()\n",
        "    cl_text2 = re.sub(r'[^\\w\\s\\.\\,\\-]','', text_2)\n",
        "    cl_text2 = cl_text2.replace(' - ', ' ')\n",
        "grams2 = get_first_4grams(cl_text2)\n",
        "grams_preps2 = get_th_grams(get_grams_with_preps(grams2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmqvCP2RJozT",
        "outputId": "a66f3598-4b54-4cce-fe81-357fe4dd6be1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 24min 57s, sys: 12 s, total: 25min 9s\n",
            "Wall time: 25min 2s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "with open(\"/content/drive/MyDrive/news_texts/2010.txt\", \"r\", encoding = \"utf-8\") as c:\n",
        "    text_3 = c.read()\n",
        "    cl_text3 = re.sub(r'[^\\w\\s\\.\\,\\-]','', text_3)\n",
        "    cl_text3 = cl_text3.replace(' - ', ' ')\n",
        "grams3 = get_first_4grams(cl_text3)\n",
        "grams_preps3 = get_th_grams(get_grams_with_preps(grams3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLyFCefjKVkH",
        "outputId": "82e5aa93-b297-4647-da69-792225080fd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 26min 7s, sys: 12.8 s, total: 26min 20s\n",
            "Wall time: 26min 13s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "with open(\"/content/drive/MyDrive/news_texts/2011.txt\", \"r\", encoding = \"utf-8\") as d:\n",
        "    text_4 = d.read()\n",
        "    cl_text4 = re.sub(r'[^\\w\\s\\.\\,\\-]','', text_4)\n",
        "    cl_text4 = cl_text4.replace(' - ', ' ')\n",
        "grams4 = get_first_4grams(cl_text4)\n",
        "grams_preps4 = get_th_grams(get_grams_with_preps(grams4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niqXKTzcKkJw",
        "outputId": "1b774729-1b65-4215-a718-790ef8a4ddbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 26min 44s, sys: 14.5 s, total: 26min 58s\n",
            "Wall time: 26min 51s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "with open(\"/content/drive/MyDrive/news_texts/2012.txt\", \"r\", encoding = \"utf-8\") as e:\n",
        "    text_5 = e.read()\n",
        "    cl_text5 = re.sub(r'[^\\w\\s\\.\\,\\-]','', text_5)\n",
        "    cl_text5 = cl_text5.replace(' - ', ' ')\n",
        "grams5 = get_first_4grams(cl_text5)\n",
        "grams_preps5 = get_th_grams(get_grams_with_preps(grams5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PSwnKKAKzU_",
        "outputId": "dad66d14-2534-4481-9d70-3fd5d80c5168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 26min 59s, sys: 13.8 s, total: 27min 13s\n",
            "Wall time: 27min 7s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "with open(\"/content/drive/MyDrive/news_texts/2013.txt\", \"r\", encoding = \"utf-8\") as ab:\n",
        "    text_6 = ab.read()\n",
        "    cl_text6 = re.sub(r'[^\\w\\s\\.\\,\\-]','', text_6)\n",
        "    cl_text6 = cl_text6.replace(' - ', ' ')\n",
        "grams6 = get_first_4grams(cl_text6)\n",
        "grams_preps6 = get_th_grams(get_grams_with_preps(grams6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjOi6b2iKz-D",
        "outputId": "087d747b-91d4-453a-9bd5-92c3bd4820a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 28min 24s, sys: 14.7 s, total: 28min 39s\n",
            "Wall time: 28min 33s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "with open(\"/content/drive/MyDrive/news_texts/2014.txt\", \"r\", encoding = \"utf-8\") as cd:\n",
        "    text_7 = cd.read()\n",
        "    cl_text7 = re.sub(r'[^\\w\\s\\.\\,\\-]','', text_7)\n",
        "    cl_text7 = cl_text7.replace(' - ', ' ')\n",
        "grams7 = get_first_4grams(cl_text7)\n",
        "grams_preps7 = get_th_grams(get_grams_with_preps(grams7))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KL5E2SmiLHIh",
        "outputId": "893b416c-8b08-41a5-9298-1f88ab576d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 27min 3s, sys: 13.8 s, total: 27min 17s\n",
            "Wall time: 27min 11s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "with open(\"/content/drive/MyDrive/news_texts/2015.txt\", \"r\", encoding = \"utf-8\") as ef:\n",
        "    text_8 = ef.read()\n",
        "    cl_text8 = re.sub(r'[^\\w\\s\\.\\,\\-]','', text_8)\n",
        "    cl_text8 = cl_text8.replace(' - ', ' ')\n",
        "grams8 = get_first_4grams(cl_text8)\n",
        "grams_preps8 = get_th_grams(get_grams_with_preps(grams8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNFOmoxCLHwR",
        "outputId": "d25716ea-084a-41cf-f39f-4885f32aff03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 26min 21s, sys: 13.3 s, total: 26min 35s\n",
            "Wall time: 26min 28s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "with open(\"/content/drive/MyDrive/news_texts/2016.txt\", \"r\", encoding = \"utf-8\") as f:\n",
        "    text_9 = f.read()\n",
        "    cl_text9 = re.sub(r'[^\\w\\s\\.\\,\\-]','', text_9)\n",
        "    cl_text9 = cl_text9.replace(' - ', ' ')\n",
        "grams9 = get_first_4grams(cl_text9)\n",
        "grams_preps9 = get_th_grams(get_grams_with_preps(grams9))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYcaeU4NLIaf",
        "outputId": "034db08a-504d-46c0-91a1-82e59f5a0d5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 23min 24s, sys: 11.9 s, total: 23min 35s\n",
            "Wall time: 23min 29s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "with open(\"/content/drive/MyDrive/news_texts/2017.txt\", \"r\", encoding = \"utf-8\") as g:\n",
        "    text_10 = g.read()\n",
        "    cl_text10 = re.sub(r'[^\\w\\s\\.\\,\\-]','', text_10)\n",
        "    cl_text10 = cl_text10.replace(' - ', ' ')\n",
        "grams10 = get_first_4grams(cl_text10)\n",
        "grams_preps10 = get_th_grams(get_grams_with_preps(grams10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgM7yoO0Lghe",
        "outputId": "afe3c26f-99f7-4633-bdb6-bb8360eb7003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 26min 31s, sys: 13.3 s, total: 26min 44s\n",
            "Wall time: 26min 39s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "with open(\"/content/drive/MyDrive/news_texts/Interfax_texts.txt\", \"r\", encoding = \"utf-8\") as fg:\n",
        "    text_11 = fg.read()\n",
        "    cl_text11 = re.sub(r'[^\\w\\s\\.\\,\\-]','', text_11)\n",
        "    cl_text11 = cl_text11.replace(' - ', ' ')\n",
        "grams11 = get_first_4grams(cl_text11)\n",
        "grams_preps11 = get_th_grams(get_grams_with_preps(grams11))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWOytqWBLwq_",
        "outputId": "feb975fd-51b0-4dad-eea0-dedc4b0c280f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 26min 8s, sys: 13.1 s, total: 26min 21s\n",
            "Wall time: 26min 13s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "with open(\"/content/drive/MyDrive/news_texts/KP_texts.txt\", \"r\", encoding = \"utf-8\") as h:\n",
        "    text_12 = h.read()\n",
        "    cl_text12 = re.sub(r'[^\\w\\s\\.\\,\\-]','', text_12)\n",
        "    cl_text12 = cl_text12.replace(' - ', ' ')\n",
        "grams12 = get_first_4grams(cl_text12)\n",
        "grams_preps12 = get_th_grams(get_grams_with_preps(grams12))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBshkGdUMDbk",
        "outputId": "fd1c215c-d8e2-401c-c8d6-4a4ba5e74091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 37min 16s, sys: 18.5 s, total: 37min 34s\n",
            "Wall time: 37min 26s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "with open(\"/content/drive/MyDrive/news_texts/Lenta_texts.txt\", \"r\", encoding = \"utf-8\") as fl:\n",
        "    text_13 = fl.read()\n",
        "    cl_text13 = re.sub(r'[^\\w\\s\\.\\,\\-]','', text_13)\n",
        "    cl_text13 = cl_text13.replace(' - ', ' ')\n",
        "grams13 = get_first_4grams(cl_text13)\n",
        "grams_preps13 = get_th_grams(get_grams_with_preps(grams13))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объединяем все: \n",
        "\n",
        "*Безусловно, все можно было объединить сразу, но так кажется менее громоздко*"
      ],
      "metadata": {
        "id": "M-Y5wEA8u0Zx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36IzRa9S4xES"
      },
      "outputs": [],
      "source": [
        "prep_grams_part1 = grams_preps0 + grams_preps1 + grams_preps2 + grams_preps3 + grams_preps4\n",
        "prep_grams_part2 = grams_preps5 + grams_preps6 + grams_preps7 + grams_preps8\n",
        "prep_grams_part3 = grams_preps9 + grams_preps10 + grams_preps11 + grams_preps12 + grams_preps13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKL0H27Y6xGA"
      },
      "outputs": [],
      "source": [
        "prep_grams = prep_grams_part1 + prep_grams_part2 + prep_grams_part3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Удаляем глаголы:"
      ],
      "metadata": {
        "id": "TPToJNqMyTAx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELPYOsSJ7XX9"
      },
      "outputs": [],
      "source": [
        "phrase_poses = {}\n",
        "for gram in prep_grams:\n",
        "    doc = nlp(gram)\n",
        "    phrase_poses[doc] = []\n",
        "    for token in doc:\n",
        "        phrase_poses[doc].append(token.pos_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lW3jIySn7bMN"
      },
      "outputs": [],
      "source": [
        "grams_novbs = []\n",
        "for phr, poses in phrase_poses.items():\n",
        "    col = []\n",
        "    if \"VERB\" in poses:\n",
        "        for token in phr:\n",
        "            if token.pos_ != \"VERB\": \n",
        "                col.append(str(token))\n",
        "        grams_novbs.append(\" \".join(col))\n",
        "    else:\n",
        "        grams_novbs.append(phr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дополнительно очищаем коллокации от запятых и разных аномалий:"
      ],
      "metadata": {
        "id": "8lHwSZZ5vMnT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8-WSLu17d3K"
      },
      "outputs": [],
      "source": [
        "cl_grams = []\n",
        "for gr in grams_novbs:\n",
        "    clgr = str(gr).replace(\",\", \" \")\n",
        "    clgr = clgr.replace(\"  \", \" \")\n",
        "    clgr = clgr.replace(\"   \", \" \")\n",
        "    clgr = clgr.replace(\"  как\", \" как\")\n",
        "    clgr = clgr.replace(\"  чтобы\", \" чтобы\")\n",
        "    if clgr.endswith(\" \"):\n",
        "        clgr = clgr[:-1]\n",
        "    cl_grams.append(clgr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем более короткие n-граммы, в которых тоже ищутся предлоги и анафоры:"
      ],
      "metadata": {
        "id": "oaGoGCUXrsCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eksp = []\n",
        "for i in cl_grams:\n",
        "    i_spl = i.split()\n",
        "    if len(i_spl) == 4:\n",
        "        twogr = \" \".join(i_spl[:2])\n",
        "        thrgr = \" \".join(i_spl[:3])\n",
        "        eksp.append(twogr)\n",
        "        eksp.append(thrgr)\n",
        "    elif len(i_spl) == 3:\n",
        "        twogr = \" \".join(i_spl[:2])\n",
        "        eksp.append(twogr)"
      ],
      "metadata": {
        "id": "ruLPpwrrPAnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "th23grams = get_th_grams(get_grams_with_preps(eksp))"
      ],
      "metadata": {
        "id": "XveNhbvPnhcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pref_grams = cl_grams + th23grams"
      ],
      "metadata": {
        "id": "1rmbcaYNqssh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обращаем внимание на концовку выражений:"
      ],
      "metadata": {
        "id": "0BybSD9XviNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fingrams = []\n",
        "for gr in pref_grams:\n",
        "    doc = nlp(gr)\n",
        "    if doc[-1].pos_ == \"NOUN\" or doc[-1].pos_ == \"ADP\" or doc[-1].pos_ == \"ADJ\" or doc[-1].pos_ == \"ADV\" or doc[-1].pos_ == \"NUM\" or doc[-1].pos_ == \"X\" or doc[-1].text == \"он\":\n",
        "        pass\n",
        "    else:\n",
        "        fingrams.append(gr)"
      ],
      "metadata": {
        "id": "ayV9RZl1qzTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fgrams = [str(gr) for gr in fingrams] #позже было обнаружено, что в fingrams записались результаты формата doc, этого можно было избежать в предыдущем куске кода"
      ],
      "metadata": {
        "id": "6Sbr77FZwGDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UeIU8zlEVdi"
      },
      "outputs": [],
      "source": [
        "counter = collections.Counter(fgrams)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnt = {}\n",
        "for k, v in counter.items():\n",
        "    if v > 1: #записываем в более сокращенный словарь-счетчик выражения, встретившиеся больше одного раза\n",
        "        cnt[k] = v"
      ],
      "metadata": {
        "id": "lw9qUFzTPOlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_dict_byval(somed): #сортировка по значениям\n",
        "    sorted_tuple = sorted(somed.items(), key=lambda x: x[1], reverse=True)\n",
        "    return dict(sorted_tuple)"
      ],
      "metadata": {
        "id": "8DPjAqK06pT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Расчет C-Value, запись в файл"
      ],
      "metadata": {
        "id": "w6SOgnMzwQNo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sHCAYV2EaIh"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import operator\n",
        "from collections import OrderedDict\n",
        "\n",
        "d_with_cdata = {}\n",
        "keys = list(cnt.keys())\n",
        "\n",
        "for k, v in cnt.items():\n",
        "    vals_f_c_t = []\n",
        "    c = 0\n",
        "    t = 0\n",
        "    for k2 in keys:\n",
        "        if k in k2:\n",
        "            c += 1\n",
        "            t += cnt[k2]\n",
        "    vals_f_c_t.append(v)\n",
        "    vals_f_c_t.append(c-1)\n",
        "    vals_f_c_t.append(t-v)\n",
        "    d_with_cdata[k] = vals_f_c_t\n",
        "\n",
        "\n",
        "cndt_cval = {}\n",
        "for cndt, c_data in d_with_cdata.items():\n",
        "    f = c_data[0]\n",
        "    c = c_data[1]\n",
        "    t = c_data[2]\n",
        "    cndt_l = len(str(cndt).split())\n",
        "    if c == 0:\n",
        "        cval = math.log2(cndt_l) * f\n",
        "        cndt_cval[cndt] = float(\"%.3f\" % cval)\n",
        "    else:\n",
        "        cval = math.log2(cndt_l) * (f - 1/c * t)\n",
        "        cndt_cval[cndt] = float(\"%.3f\" % cval)\n",
        "\n",
        "res_ = sort_dict_byval(cndt_cval)\n",
        "\n",
        "with open('cval_prob1eksp.txt','w', encoding=\"utf-8\") as out:\n",
        "    for key,val in res_.items():\n",
        "        out.write('{}: {}\\n'.format(key,val))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "eksp1_1GB.ipynb\"",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}