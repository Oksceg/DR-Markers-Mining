{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Эксперимент №1\n",
        "В данном эксперименте производится попытка извлечения n-грамм по определенным заданным критериям, которые свойственны дискурсивным маркерам"
      ],
      "metadata": {
        "id": "LltJsE-Es0Jg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Скачиваем *spacy_udpipe*, импортируем нужные для работы модули"
      ],
      "metadata": {
        "id": "QLewzWibshdq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lmeqOAvOCz_",
        "outputId": "37d3db18-35a3-4a94-f62a-1f557d5c67dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy_udpipe\n",
            "  Downloading spacy_udpipe-1.0.0-py3-none-any.whl (11 kB)\n",
            "Collecting ufal.udpipe>=1.2.0\n",
            "  Downloading ufal.udpipe-1.2.0.3.tar.gz (304 kB)\n",
            "\u001b[K     |████████████████████████████████| 304 kB 13.9 MB/s \n",
            "\u001b[?25hCollecting spacy<4.0.0,>=3.0.0\n",
            "  Downloading spacy-3.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 31.9 MB/s \n",
            "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
            "Collecting typing-extensions<4.0.0.0,>=3.7.4\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.6)\n",
            "Collecting thinc<8.1.0,>=8.0.14\n",
            "  Downloading thinc-8.0.16-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (660 kB)\n",
            "\u001b[K     |████████████████████████████████| 660 kB 25.8 MB/s \n",
            "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 8.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.23.0)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 718 kB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (57.4.0)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.9.1)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (4.64.0)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 45.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.0.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.4.1)\n",
            "Collecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n",
            "\u001b[K     |████████████████████████████████| 457 kB 10.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (21.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.9)\n",
            "Collecting smart-open<6.0.0,>=5.0.0\n",
            "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.0.1)\n",
            "Building wheels for collected packages: ufal.udpipe\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp37-cp37m-linux_x86_64.whl size=5626629 sha256=31b796eb2cebe3f650740e4b63480c237b84595c833f06d3b9e6a85dd19819f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/b5/8e/3da091629a21ce2d10bf90759d0cb034ba10a5cf7a01e83d64\n",
            "Successfully built ufal.udpipe\n",
            "Installing collected packages: typing-extensions, catalogue, typer, srsly, smart-open, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, ufal.udpipe, spacy, spacy-udpipe\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.2.0\n",
            "    Uninstalling typing-extensions-4.2.0:\n",
            "      Successfully uninstalled typing-extensions-4.2.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 6.0.0\n",
            "    Uninstalling smart-open-6.0.0:\n",
            "      Successfully uninstalled smart-open-6.0.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n",
            "Successfully installed catalogue-2.0.7 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 smart-open-5.2.1 spacy-3.3.0 spacy-legacy-3.0.9 spacy-loggers-1.0.2 spacy-udpipe-1.0.0 srsly-2.4.3 thinc-8.0.16 typer-0.4.1 typing-extensions-3.10.0.2 ufal.udpipe-1.2.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy_udpipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO15J2NEr5Np",
        "outputId": "cf87e228-4a64-4552-dfd5-a1c856b046d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded pre-trained UDPipe model for 'ru' language\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import collections\n",
        "import spacy_udpipe\n",
        "spacy_udpipe.download(\"ru\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fc1jshspnTj8"
      },
      "outputs": [],
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Извлечение"
      ],
      "metadata": {
        "id": "sb9WiTSZxFrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Открываем текст, чистим его, оставляя точки, запятые, дефисы:"
      ],
      "metadata": {
        "id": "go4jisH-xPLj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrOmLWCMk80o"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/news_texts/2009.txt\", \"r\", encoding = \"utf-8\") as f:\n",
        "    text_full = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZh-qELUm1QA"
      },
      "outputs": [],
      "source": [
        "cl_text = re.sub(r'[^\\w\\s\\.\\,\\-]','', text_full)\n",
        "cl_text = cl_text.replace(' - ', ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGjWv0H7UOuA",
        "outputId": "6d3bf24b-c9cb-4cdc-d332-40104442aa09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5.91 s, sys: 102 ms, total: 6.01 s\n",
            "Wall time: 7.2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "nlp = spacy_udpipe.load(\"ru\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdvUkLE2mmS8"
      },
      "outputs": [],
      "source": [
        "#получение 4-грамм\n",
        "bgn_4grams_lists=[sent.lower().split()[:4] for sent in cl_text.split(\".\")]\n",
        "\n",
        "#сокращаем 4-граммы в зависимости от запятых: сохраняем всю часть до запятой и одно слово после нее\n",
        "bgn_grams = []\n",
        "for gram in bgn_4grams_lists:\n",
        "    gram_to_list = \"\"\n",
        "    for w in gram:\n",
        "        if \",\" in w:\n",
        "            gram_to_list = \" \".join(gram[:gram.index(w)+2])\n",
        "        else:\n",
        "            gram_to_list = \" \".join(gram)\n",
        "    bgn_grams.append(gram_to_list)\n",
        "#в общем это должно так работать, но я пока не уверена"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_grams_with_preps(grams):\n",
        "    #набираем выражения с предлогом\n",
        "    prep_grams = []\n",
        "    for gram in grams:\n",
        "        doc = nlp(gram)\n",
        "        for token in doc:\n",
        "            if token.pos_ == \"ADP\":\n",
        "                prep_grams.append(doc)\n",
        "    return prep_grams"
      ],
      "metadata": {
        "id": "3Rn367UyRcoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_th_grams(grams_):\n",
        "    #набираем выражения с анафорами\n",
        "    th_grams = []\n",
        "    for gram in grams_:\n",
        "        doc = nlp(gram)\n",
        "        for token in doc:\n",
        "            if token.lemma_ == \"это\" or token.lemma_ == \"то\":\n",
        "                if token.pos_ == \"PRON\":\n",
        "                    th_grams.append(doc)\n",
        "    return th_grams"
      ],
      "metadata": {
        "id": "GA_1IKyyntM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "th_4grams = get_th_grams(get_grams_with_preps(bgn_grams))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bijSYnDsSp1I",
        "outputId": "cfa2f3d3-ca7e-4b1f-d57c-643ffd8bdd2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 25min 45s, sys: 12.2 s, total: 25min 57s\n",
            "Wall time: 25min 54s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z_DtIrBduOa"
      },
      "source": [
        "Нужно попробовать убрать глаголы, т.к. кажется, что они редко встречаются в коннекторах"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwDCgF4ZhUll"
      },
      "outputs": [],
      "source": [
        "phrase_poses = {}\n",
        "for gram in th_4grams:\n",
        "    doc = nlp(gram)\n",
        "    phrase_poses[doc] = []\n",
        "    for token in doc:\n",
        "        phrase_poses[doc].append(token.pos_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOFcdVHmh310"
      },
      "outputs": [],
      "source": [
        "grams_novbs = []\n",
        "for phr, poses in phrase_poses.items():\n",
        "    col = []\n",
        "    if \"VERB\" in poses:\n",
        "        for token in phr:\n",
        "            if token.pos_ != \"VERB\": \n",
        "                col.append(str(token))\n",
        "        grams_novbs.append(\" \".join(col))\n",
        "    else:\n",
        "        grams_novbs.append(str(phr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Zl4dAj5Xlm"
      },
      "source": [
        "Убираем запятые и разные аномалии"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyQXj5JaPl13"
      },
      "outputs": [],
      "source": [
        "cl_grams = []\n",
        "for gr in grams_novbs:\n",
        "    clgr = gr.replace(\",\", \" \")\n",
        "    clgr = clgr.replace(\"  \", \" \")\n",
        "    clgr = clgr.replace(\"   \", \" \")\n",
        "    clgr = clgr.replace(\"  как\", \" как\")\n",
        "    clgr = clgr.replace(\"  чтобы\", \" как\")\n",
        "    if clgr.endswith(\" \"):\n",
        "        clgr = clgr[:-1]\n",
        "    cl_grams.append(clgr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8gyQ_kvcDCP"
      },
      "source": [
        "### Что теперь?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DFOIm7hcGr1"
      },
      "source": [
        "- составляем из начала каждой n-граммы соответствующие 2, 3-граммы \n",
        "- далее считаем c-value"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eksp = []\n",
        "for i in cl_grams:\n",
        "    i_spl = i.split()\n",
        "    if len(i_spl) == 4:\n",
        "        twogr = \" \".join(i_spl[:2])\n",
        "        thrgr = \" \".join(i_spl[:3])\n",
        "        eksp.append(twogr)\n",
        "        eksp.append(thrgr)\n",
        "    elif len(i_spl) == 3:\n",
        "        twogr = \" \".join(i_spl[:2])\n",
        "        eksp.append(twogr)"
      ],
      "metadata": {
        "id": "qKxFkwPQ06WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "th23grams = get_th_grams(get_grams_with_preps(eksp))"
      ],
      "metadata": {
        "id": "rwnotLdBZlm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pref_grams = cl_grams + th23grams"
      ],
      "metadata": {
        "id": "CwiCcStagcuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обращаем внимание на концовки:"
      ],
      "metadata": {
        "id": "ONG-QpG-yERL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNCd9OwC1s0Y"
      },
      "outputs": [],
      "source": [
        "fingrams = []\n",
        "for gr in pref_grams:\n",
        "    doc = nlp(gr)\n",
        "    if doc[-1].pos_ == \"NOUN\" or doc[-1].pos_ == \"ADP\" or doc[-1].pos_ == \"ADJ\" or doc[-1].pos_ == \"ADV\" or doc[-1].pos_ == \"NUM\" or doc[-1].pos_ == \"X\" or doc[-1].text == \"он\":\n",
        "        pass\n",
        "    else:\n",
        "        fingrams.append(gr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fgrams = [str(gr) for gr in fingrams]"
      ],
      "metadata": {
        "id": "ocSv7pKMR6Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osuYYYU_oo6n"
      },
      "outputs": [],
      "source": [
        "counter = collections.Counter(fgrams)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnt = {}\n",
        "for k, v in counter.items():\n",
        "    if v > 1: #записываем в более короткий словарь-счетчик выражения, встретившиеся больше одного раза\n",
        "        cnt[k] = v"
      ],
      "metadata": {
        "id": "oRqMpvj7DvyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESErtIVqocNf"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import operator\n",
        "from collections import OrderedDict\n",
        "\n",
        "def sort_dict_byval(somed):\n",
        "    sorted_tuple = sorted(somed.items(), key=lambda x: x[1], reverse=True)\n",
        "    return dict(sorted_tuple)\n",
        "\n",
        "d_with_cdata = {}\n",
        "keys = list(cnt.keys())\n",
        "\n",
        "for k, v in cnt.items():\n",
        "    vals_f_c_t = []\n",
        "    c = 0\n",
        "    t = 0\n",
        "    for k2 in keys:\n",
        "        if str(k) in str(k2):\n",
        "            c += 1\n",
        "            t += cnt[k2]\n",
        "    vals_f_c_t.append(v)\n",
        "    vals_f_c_t.append(c-1)\n",
        "    vals_f_c_t.append(t-v)\n",
        "    d_with_cdata[k] = vals_f_c_t\n",
        "\n",
        "\n",
        "cndt_cval = {}\n",
        "for cndt, c_data in d_with_cdata.items():\n",
        "    f = c_data[0]\n",
        "    c = c_data[1]\n",
        "    t = c_data[2]\n",
        "    cndt_l = len(str(cndt).split())\n",
        "    if c == 0:\n",
        "        cval = math.log2(cndt_l) * f\n",
        "        cndt_cval[cndt] = float(\"%.3f\" % cval)\n",
        "    else:\n",
        "        cval = math.log2(cndt_l) * (f - 1/c * t)\n",
        "        cndt_cval[cndt] = float(\"%.3f\" % cval)\n",
        "\n",
        "res_ = sort_dict_byval(cndt_cval)\n",
        "\n",
        "with open('short_cval_1eksp.txt','w', encoding=\"utf-8\") as out:\n",
        "    for key,val in res_.items():\n",
        "        out.write('{}: {}\\n'.format(key,val))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "eksp1_77MB.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}